[
  {
    "objectID": "posts/从理论断层到统一框架：LLM强化学习对齐算法深度研究（一）/index.html",
    "href": "posts/从理论断层到统一框架：LLM强化学习对齐算法深度研究（一）/index.html",
    "title": "LLM推理的“瘦身革命”：少即是多，还是新的枷索？——深入剖析GFPO算法及其数学原理",
    "section": "",
    "text": "记录了一次关于大型语言模型（LLM）强化学习（RL）对齐算法的深度研讨。我们从一个基础性的矛盾出发——即在LLM这种非遍历性序列生成任务中，Token级重要性采样的理论不自洽性。以此为起点，我们系统性地重演了从策略梯度第一性原理到现代实用算法（如PPO, GRPO）的推导路径，并揭示了其中为了实践可行性而做出的关键妥协，如序列级重要性采样（IS）的引入及其内在的方差-偏差困境。我们深入剖析了GRPO等算法存在的长度与难度偏差，并审视了DAPO和CISPO等前沿工作如何从工程和理论层面修复这些缺陷。最后，基于本次研讨的洞察，我们提出了一个旨在统一现有算法优点、并解决其核心痛点的综合性框架（SGS-CISPO），并对其数学完备性和实践潜力进行了严谨评估。\n\n第一幕： foundational Conflict - 理论的优雅与现实的诅咒\n一切LLM-RLHF的起点，都是最大化期望奖励这一简单而优美的目标： \\[\nJ(\\theta) = \\mathbb{E}_{o \\sim P_\\theta(o)}[R(o)]\n\\] 其中，\\(P_\\theta(o)\\)是策略模型\\(\\pi_\\theta\\)生成完整序列\\(o\\)的概率。通过应用Log-Derivative Trick，我们得到了著名的REINFORCE策略梯度： \\[\n\\nabla_\\theta J(\\theta) = \\mathbb{E}_{o \\sim P_\\theta(o)}[R(o) \\nabla_\\theta \\log P_\\theta(o)]\n\\] 然而，这个理论上完美的梯度在实践中存在两大缺陷：高方差和On-Policy数据低效。为了解决这两个问题，我们引入了两个标准工具：\n\n基线（Baseline）: 引入优势函数 \\(\\hat{A}(o) = R(o) - b\\) 来降低方差。\n重要性采样（Importance Sampling, IS）: 允许使用旧策略\\(\\pi_{\\text{old}}\\)采样的数据，以提高效率。\n\n将两者结合，我们得到了理论上最完备的Off-Policy策略梯度： \\[\n\\nabla_\\theta J(\\theta) = \\mathbb{E}_{o \\sim P_{\\text{old}}(o)}\\left[ \\frac{P_\\theta(o)}{P_{\\text{old}}(o)} \\cdot \\hat{A}(o) \\cdot \\nabla_\\theta \\log P_\\theta(o) \\right]\n\\] 这里的核心是序列级重要性采样比例： \\[\nR_{\\text{sequence}}(o) = \\frac{P_\\theta(o)}{P_{\\text{old}}(o)} = \\prod_{t=1}^{T} \\frac{\\pi_\\theta(o_t|o_{&lt;t})}{\\pi_{\\text{old}}(o_t|o_{&lt;t})} = \\prod_{t=1}^{T} \\rho_t\n\\] 至此，我们遭遇了第一个，也是最根本的理论-实践断裂点。\n这个连乘积\\(R_{\\text{sequence}}(o)\\)，虽然在数学上是唯一正确的权重，但在实践中是彻头彻尾的灾难。它的方差会随着序列长度\\(T\\)指数级爆炸，并导致毁灭性的数值不稳定性（梯度消失或爆炸）。理论的圣杯，在现实中却“有毒”。\n\n\n第二幕：The Age of Compromise - PPO/GRPO/GSPO的问题\n为了让训练能够进行，现代算法必须对\\(R_{\\text{sequence}}(o)\\)这个“有毒的圣杯”进行“解毒”。这催生了两种主流的妥协方案：\n方案A：PPO/GRPO的“理论混搭”\nPPO及其变体GRPO，采取了一种极其务实但理论上不纯粹的方案。它们放弃了序列级的\\(R_{\\text{sequence}}\\)，转而直接在Token级别上使用IS： \\[\nL_{\\text{PPO/GRPO}} \\propto - \\sum_{i,t} \\min\\left( \\rho_{i,t} \\cdot \\hat{A}_i, \\text{clip}(\\rho_{i,t}) \\hat{A}_i \\right)\n\\] 这里的核心问题是，它将一个序列级的优势函数\\(\\hat{A}_i\\)与一个Token级的重要性采样比例\\(\\rho_{i,t}\\)直接相乘。这在“序列即动作”的理论框架下是不自洽的，它造成了理论上的断层，但其简单的形式和鲁棒的稳定性使其成为业界标准。\n方案B：GSPO的“有偏妥协”\nGSPO试图在理论上做得更自洽。它坚守“序列即动作”的原则，但为了解决\\(R_{\\text{sequence}}\\)的方差问题，它采用了几何平均来替代： \\[\nG(o) = (R_{\\text{sequence}}(o))^{1/T} = \\left(\\prod_{t=1}^{T} \\rho_t\\right)^{1/T}\n\\] 这是第二个关键的妥协点。几何平均通过在对数空间取算术平均（\\(\\log G(o) = \\frac{1}{T}\\sum \\log \\rho_t\\)），成功地将指数增长的方差驯服为以\\(1/T\\)速率下降的方差。\n\n合理性: 保留了连乘的本质结构（最弱一环效应），且极大地稳定了训练。\n不合理性: 引入了系统性的、向下的偏差。根据AM-GM不等式，几何平均总是小于等于算术平均，这意味着它会系统性地低估真实的IS权重，尤其是在新旧策略差异较大时。\n\n此外，早期的GRPO还存在两个严重的工程缺陷，后被DAPO等工作修正： 1. 长度偏差: 由于对每个序列的损失进行平均（\\(1/|o_i|\\)），导致长序列中关键token的梯度信号被稀释。解决方案是采用Token级归一化（分母为总token数）。 2. 难度偏差: 由于对优势函数按组进行标准化（\\(/std()\\)），导致全对/全错（低方差）的组被赋予过高权重，且在std=0时梯度爆炸。解决方案是动态采样，过滤掉这些组。\n\n\n第三幕：The Reformation - CISPO的“手术刀式”修复\n在PPO/GRPO的框架下，clip机制本身也存在一个长期被忽视的缺陷。当一个关键token的IS比例\\(\\rho_t\\)因其重要性而远超\\(1+\\epsilon\\)时，PPO的损失项变为一个真正常量，其梯度瞬间归零。这扼杀了模型学习突破性创新的能力。\nCISPO通过一个极其精妙的stop_gradient操作，完美地修复了这个问题。其目标函数形如： \\[\nJ_{\\text{CISPO}}(\\theta) \\propto \\mathbb{E} \\left[ \\text{sg}(\\text{clip}(\\rho_{i,t})) \\cdot \\hat{A}_{i,t} \\cdot \\log \\pi_\\theta(o_{i,t}|...) \\right]\n\\] 其梯度为： \\[\n\\nabla_\\theta J_{\\text{CISPO}} \\propto \\text{clip}(\\rho_{i,t}) \\cdot \\hat{A}_{i,t} \\cdot \\nabla_\\theta \\log \\pi_\\theta(o_{i,t}|...)\n\\] 在这里，clip(rho)不再是梯度的“开关”，而是一个无梯度的、有界的“缩放因子”。梯度永远不会被杀死，只是其大小被限制了。CISPO不是PPO的变体，而是一个方差有界的、稳定的REINFORCE算法，它同时解决了REINFORCE的梯度爆炸和PPO的梯度消失问题。\n\n\n第四幕：The Synthesis - 一个统一、更优的框架构想\n基于以上所有讨论，我们能否设计一个集大成、同时解决各自痛点的统一框架？我们的研讨提出了这样一个构想，它建立在几个核心原则之上：\n原则一：地基——采纳DAPO的最佳实践。 * 默认使用Token级归一化消除长度偏差。 * 默认使用动态采样消除难度偏差。\n原则二：引擎——采用CISPO的稳定梯度机制。 * 抛弃PPO的clip，使用CISPO的stop_gradient机制，确保梯度信号的完整性和稳定性。\n原则三：哲学——引入平滑梯度缩放（SGS），实现智能的正负优化。 我们认识到，单纯的负向优化（如LoNSPo）存在学习效率低下和能力上限受限的问题。而单纯的正向优化又面临“过度模仿”和“熵坍塌”的风险。为此，我们设计了一个平滑梯度缩放（Smooth Gradient Scaling, SGS）机制。\n其核心是设计一个依赖于模型置信度的梯度缩放因子 \\(S(P_\\theta, \\hat{A})\\): \\[\nS(P_\\theta, \\hat{A}) =\n\\begin{cases}\n1 & \\text{if } \\hat{A} \\le 0 \\\\\n1 - \\sigma(k(P_\\theta(o) - \\tau)) & \\text{if } \\hat{A} &gt; 0\n\\end{cases}\n\\] 这个机制在数学上可以证明： 1. 对于负样本 (\\(\\hat{A} \\le 0\\)): 它不施加任何抑制，执行完全的“进化选择”，有效淘汰错误。 2. 对于正样本 (\\(\\hat{A} &gt; 0\\)): 它允许对低置信度（新探索）的样本进行充分学习，但随着模型置信度\\(P_\\theta\\)的提升，会平滑地抑制梯度，从而在利用正样本的同时，从机制上防止了熵坍塌，避免了对已有知识的“过度学习”。\n最终的统一损失函数框架 (SGS-CISPO): \\[\nL_{\\text{SGS-CISPO}}(\\theta) = - \\frac{1}{\\sum|o_i|} \\sum_{i,t} \\left[ S_{i,t} \\cdot \\text{sg}(\\text{clip}(\\rho_{i,t})) \\cdot \\hat{A}_{i,t} \\cdot \\log \\pi_\\theta(o_{i,t}|...) \\right]\n\\] 其中，\\(S_{i,t}\\)是应用于每个token的SGS缩放因子，\\(\\hat{A}_{i,t}\\)可采用LoNSPo的留一法基线以获得更精确的信号。\n\n\n结论：迈向更完备的对齐理论\n我们的研讨从一个根本性的理论矛盾出发，层层剖析了现代LLM-RLHF算法在实践中做出的种种妥协及其代价。从PPO/GRPO的理论断层，到GSPO的有偏近似，再到DAPO和CISPO的精细修复，我们看到了一条清晰的演进路径：算法正朝着更稳定、更高效、理论更完备的方向发展。\n我们最终提出的SGS-CISPO框架，正是这一演进方向的逻辑延伸。它试图在一个统一的数学形式下，同时解决梯度稳定性（CISPO）、偏差（DAPO）和探索-利用的深层矛盾（SGS）。这是否是最终的答案尚不可知，但它为未来的研究指明了一条充满希望的、值得探索的道路。"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mythos",
    "section": "",
    "text": "LLM推理的“瘦身革命”：少即是多，还是新的枷索？——深入剖析GFPO算法及其数学原理\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nAug 15, 2025\n\n\n杨诚操\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nAug 15, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\nLLM推理的“瘦身革命”：少即是多，还是新的枷索？——深入剖析GFPO算法及其数学原理\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nAug 15, 2025\n\n\n杨诚操\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nAug 12, 2025\n\n\nTristan O’Malley\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/LLM推理的瘦身革命：少即是多，还是新的枷索？深入剖析GFPO算法及其数学原理/index.html",
    "href": "posts/LLM推理的瘦身革命：少即是多，还是新的枷索？深入剖析GFPO算法及其数学原理/index.html",
    "title": "LLM推理的“瘦身革命”：少即是多，还是新的枷索？——深入剖析GFPO算法及其数学原理",
    "section": "",
    "text": "在追求更强大、更聪明的语言模型（LLM）的道路上，我们常常陷入一个怪圈：模型在解决复杂推理问题时，似乎越来越“话痨”。它们生成的答案越来越长，虽然有时能换来准确率的提升，但更多时候，这些冗长的“思考过程”充满了重复、犹豫和无关紧要的填充物。这不仅拖慢了推理速度，也增加了我们对模型可控性的担忧。\n最近，一篇名为《Sample More to Think Less》的论文提出了一种名为GFPO (Group Filtered Policy Optimization) 的新方法，直指这一“长度膨胀”的痛点。它宣称能让模型在训练时“多采样”，从而在推理时“少思考”，实现了一场推理效率的“瘦身革命”。\n这听起来很美妙，但它真的如此完美吗？它是否只是用一种新的偏见替换了旧的问题？在这篇博客中，我们将以一名严谨研究者的视角，从核心原理到潜在风险，再到未来的可能性，对GFPO进行一次彻底的剖析。\n\n一、问题的核心：GRPO的“内卷”与数学瓶颈\n要理解GFPO，我们必须先了解它的前辈——GRPO (Group Relative Policy Optimization)。GRPO是当前主流的强化学习对齐算法之一，它通过让模型为每个问题生成一组（Group）答案，并基于这组答案内部的相对好坏（奖励高低）来进行学习，从而摆脱了对独立价值模型的依赖。\nGRPO的目标是最大化一个代理目标函数，其核心是优势函数\\(A_i\\)的定义： \\[\nA_i = \\frac{R(q, o_i) - \\operatorname{mean}_{j=1}^G\\{R(q, o_j)\\}}{\\operatorname{std}_{j=1}^G\\{R(q, o_j)\\}}\n\\] 其中，\\(R(q, o_i)\\)是响应\\(o_i\\)的总奖励，\\(G\\)是组内样本总数。这个公式巧妙地利用组内均值和标准差对奖励进行归一化，从而得到一个相对的“优势”分数。\n然而，这个设计有一个意想不到的副作用。在优化模型以获得更高奖励（例如，解题正确）的过程中，它往往会无意中鼓励更长的答案。因为一个详尽的、包含多种尝试的答案，更有可能“撞对”最终的正确解。这种机制导致了模型的“内卷”：为了提高正确率，不惜一切代价增加思考步骤，最终导致响应长度的失控。\n\n\n二、GFPO的巧妙一击：从“奖励塑造”到“数据筛选”\nGFPO的解决方案出人意料地简单。它在GRPO的基础上增加了一个看似微不足道却至关重要的步骤：过滤 (Filtering)。\n其工作流程如下： 1. 采样更多 (Sample More): 针对一个问题，不再只生成\\(G\\)个答案，而是生成更多，比如\\(G'\\)个（\\(G' &gt; G\\)）。 2. 设定标准 (Define Metric): 设定一个我们关心的“好”答案的标准metric(·)，这个标准可以独立于最终的任务奖励\\(R\\)。论文中主要使用了两个标准：响应长度（越短越好）和Token效率（\\(R(o)/\\text{length}(o)\\)，越高越好）。 3. 无情筛选 (Filter): 根据metric(·)，从生成的\\(G'\\)个答案中，只挑选出最符合标准的前\\(k\\)个，构成精英子集\\(S\\)。 4. 专注学习 (Focus & Learn): 完全抛弃那些不属于\\(S\\)的答案，只在被选中的这\\(k\\)个“精英”答案内部进行GRPO式的相对学习。\n这套机制的精髓在于，它将对“简洁性”的追求，从复杂的奖励函数设计中解耦出来，转化成了一个简单粗暴的“数据选择”问题。\n数学上，这是通过一个掩码 (Mask) \\(m_i\\) 实现的。GFPO修改了优势函数的计算方式： \\[\nA_{i}^{(m)} = \\frac{R(q, o_i) - \\mu_S}{\\sigma_S} \\cdot m_i\n\\] 其中： * \\(m_i = \\mathbb{I}\\{o_i \\in S\\}\\)，即如果\\(o_i\\)在精英子集\\(S\\)中，\\(m_i=1\\)，否则\\(m_i=0\\)。 * \\(\\mu_S\\)和\\(\\sigma_S\\)分别是精英子集\\(S\\)内部的奖励均值和标准差。\n对于被抛弃的样本，\\(m_i=0\\)，因此它们的优势函数\\(A_{i}^{(m)}\\)被强制设为0。在策略梯度算法中，策略更新的梯度\\(\\nabla_\\theta J\\)正比于优势函数： \\[\n\\nabla_\\theta J(\\theta) \\approx \\mathbb{E} \\left[ A^{(m)} \\cdot \\nabla_\\theta \\log \\pi_\\theta(o) \\right]\n\\] 当优势函数\\(A^{(m)}\\)为0时，该样本对梯度的贡献也精确为0——它在参数更新中被彻底“无视”了。\n\n\n三、深刻的质疑：这是优化还是“偏见注入”？\nGFPO在实验中取得了显著成功，它在大幅削减响应长度的同时，几乎没有损失准确率。但作为批判性的思考者，我们必须提出质疑：这种“成功”的代价是什么？\n我们的深度对话揭示了GFPO方法论中一个深刻的内在矛盾：它本质上是在系统性地丢弃大量负样本。\n这引发了两个核心问题： 1. 对RL逻辑的挑战：强化学习的核心是“试错”，即从成功和失败中共同学习。GFPO拒绝从它认为“不符合标准”的失败（甚至某些成功）案例中学习，这是否违背了RL的基本原则？\n\n策略空间的扭曲：GFPO的过滤行为，实际上抬高了“好”答案的基线（用\\(\\mu_S\\)代替\\(\\mu_G\\)）。一个在全局看起来还不错的答案，在“精英圈”里可能就成了差生，从而受到惩罚。这会推动模型的策略空间向一个被特定metric严重“扭曲”的区域收敛。\n\n我们的结论是：GFPO并非一个纯粹的优化算法，而是一种强大的、隐式的“偏见注入”工具。 它将我们对“简洁性”的偏好，通过数据选择的方式，强行注入到模型的学习过程中。这种“扭曲”是有意为之的，其目标就是塑造一个在特定约束下表现更优的模型。\n\n\n四、从修正到超越：GFPO范式的未来\n认识到GFPO的本质后，我们可以从两个方向探索其未来：\n1. 如何修正其“粗暴”？ GFPO的“硬截断”（\\(A^{(m)}=0\\)）丢弃了太多信息。我们可以设计更“温柔”的修正方案： * 软性惩罚: 对于被抛弃的样本\\(o_j \\notin S\\)，不将优势设为0，而是给一个固定的负值\\(A_{j}^{(m)} = -\\lambda_{rej}\\)。这样既保留了负反馈信号，又明确了对不符合metric行为的惩罚。 * 分层学习: 对“精英圈”\\(S\\)和“圈外”\\(S^c\\)的样本使用不同的学习基准和目标。例如，圈内用\\(A_{i}^{(m)} = \\frac{R(q, o_i) - \\mu_S}{\\sigma_S}\\)，圈外用\\(A_{j}^{(m)} = \\frac{R(q, o_j) - \\mu_G}{\\sigma_G} - \\delta\\)，从而充分利用所有样本信息。 * 退火机制: 在训练初期让模型自由探索（接近GRPO），后期再逐渐加强过滤偏见（接近GFPO），实现更稳定的学习。\n2. 如何利用其“偏见”？ 既然GFPO是注入偏见的利器，它的应用就可以远不止于“瘦身”。 * 多目标优化: 我们可以设计一个融合了简洁性、事实性、安全性的复杂metric函数，用GFPO这一简单框架实现复杂的多目标对齐。 * 安全对齐: 使用安全分类器作为metric，GFPO可以强力推动模型规避有害内容的生成。 * 高质量偏好学习: 与DPO等方法结合，先用GFPO筛选出高质量的候选答案，再在这些“优等生”中进行细微的偏好学习，从而训练出更具辨别力的模型。\n\n\n结论：一把双刃剑\nGFPO无疑是LLM对齐领域一项巧妙且务实的工作。它以一种全新的视角，将复杂的“奖励工程”问题转化为了更易于操作的“数据选择”问题，并取得了显著的工程效果。\n然而，我们必须清醒地认识到，GFPO是一把双刃剑。它所注入的“偏见”在当前看来利大于弊，成功为臃肿的LLM推理实现了“瘦身”。但这种对部分经验的“选择性失明”，可能会成为模型通往更高级、更鲁棒通用智能的“新枷锁”。\n理解GFPO，不仅是学习一种新算法，更是引发我们对人与机器对齐方式的深刻反思：我们是该设计一个完美的奖励函数去引导它，还是该为它设定一个严格的“朋友圈”，让它在潜移默化中学会我们所期望的样子？GFPO，为后一种可能性打开了一扇引人深思的大门。"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  }
]