[
  {
    "objectID": "posts/深入剖析Deltaformer，探寻超越Transformer的计算新范式/index.html",
    "href": "posts/深入剖析Deltaformer，探寻超越Transformer的计算新范式/index.html",
    "title": "Transformer计算能力等价于\\(TC^0\\)",
    "section": "",
    "text": "为什么标准自注意力机制的计算能力等价于\\(TC^0\\)，这意味着什么，以及这个结论是如何被证明的。\n我们将分三步来完成这个教学过程： 1. 第一步：精确定义战场——什么是自注意力（Self-Attention）和\\(TC^0\\)？ 2. 第二步：核心论证——如何用\\(TC^0\\)电路构建一个自注意力层？ 3. 第三步：深刻启示——“等价于\\(TC^0\\)”对我们意味着什么？\n\n\n第一步：精确定义战场\n在证明两者等价之前，我们必须对它们有精确、无歧义的理解。\n\n1.1 自注意力机制的数学本质\n我们通常说的自注意力机制，其核心是缩放点积注意力 (Scaled Dot-Product Attention)。其数学公式为： \\[\n\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V\n\\] 其中，\\(Q\\) (Query), \\(K\\) (Key), \\(V\\) (Value) 是输入序列\\(X\\)经过线性变换得到的矩阵。假设输入序列长度为\\(n\\)，嵌入维度为\\(d_{model}\\)，注意力头的维度为\\(d_k\\)，那么\\(Q, K, V\\)都是\\(n \\times d_k\\)的矩阵。\n让我们把这个公式分解为一系列基础的算术运算： 1. 矩阵乘法: \\(QK^T\\) (计算注意力分数)。 2. 逐元素缩放: 除以\\(\\sqrt{d_k}\\)。 3. Softmax函数: 对注意力分数进行归一化。Softmax本身包含： a. 逐元素指数运算: \\(\\exp(\\cdot)\\)。 b. 沿行求和: \\(\\sum(\\cdot)\\)。 c. 逐元素除法: \\((\\cdot) / \\sum(\\cdot)\\)。 4. 矩阵乘法: 将归一化后的分数矩阵与\\(V\\)相乘。\n整个自注意力机制，本质上就是由这些有限的、固定的算术运算构成的序列。\n\n\n1.2 \\(TC^0\\)：一种强大的并行计算模型\n\\(TC^0\\) 是计算复杂性理论中的一个类别。为了理解它，我们需要拆解它的名字：\n\nT (Threshold - 阈值): 电路不仅包含基础的AND, OR, NOT门，更关键的是包含了阈值门 (Threshold Gate)。一个阈值门接收\\(m\\)个二进制输入\\(x_1, \\dots, x_m\\)，每个输入有一个对应的整数权重\\(w_1, \\dots, w_m\\)。它还有一个整数阈值\\(T\\)。其输出为1，当且仅当输入的加权和不小于阈值： \\[\n\\text{THR}(x_1, \\dots, x_m) = 1 \\iff \\sum_{i=1}^m w_i x_i \\geq T\n\\] 否则输出为0。阈值门极其强大，一个门就可以完成多数表决（MAJ）、求和比较等复杂操作。\nC (Constant - 常数): 电路的深度 (Depth) 是一个常数，即\\(O(1)\\)。深度是指从任何输入到最终输出所经过的最长路径上的门数量。常数深度意味着计算不依赖于输入规模\\(n\\)的增长而增加串行步骤。这是大规模并行化的理论基础。\n⁰ (指数为0): 这是一个历史命名习惯的遗留，我们主要关注TC即可。电路的大小 (Size)，即总门数，是输入比特数的多项式函数，即\\(poly(n)\\)。\n\n总结一下\\(TC^0\\)的画像：它是一个计算能力很强的并行计算模型。它可以在固定的、极浅的计算深度内，利用海量（但不是无限）的阈值门，完成复杂的计算任务。\n\n\n\n\n第二步：核心论证——用\\(TC^0\\)电路模拟自注意力\n证明的关键在于，我们需要展示上述自注意力的所有算术运算，都可以在有限精度下，由一个\\(TC^0\\)电路来实现。这里的“有限精度”非常重要，因为电路处理的是二进制位，我们需要将浮点数表示为定点数或某种二进制编码。\n我们来逐一构建：\n\n2.1 基础算术运算的\\(TC^0\\)实现\n在计算理论中，已经证明了基础的算术运算（加、减、乘、除）对于有限精度的二进制数，都可以在\\(TC^0\\)中完成。 * 加法和乘法: 两个\\(k\\)-bit数字的乘法和加法可以由深度为常数的电路完成。例如，乘法可以被分解为多个部分的和，而求和可以用一个深度为\\(O(1)\\)的阈值门网络高效完成。 * 除法: 稍微复杂，但同样可以通过牛顿-拉弗森迭代等数值方法进行多项式近似，最终用\\(TC^0\\)电路实现。\n结论1: 自注意力公式中所有基础的矩阵乘法、缩放、除法，都可以被\\(TC^0\\)电路模块实现。\n\n\n2.2 Softmax函数的\\(TC^0\\)实现：最关键的一步\nSoftmax中的指数函数\\(exp(x)\\)和求和是核心难点。\n\n模拟指数函数 \\(exp(x)\\): 我们无法在电路中实现完美的\\(exp(x)\\)。但是，我们可以用多项式逼近它。例如，使用泰勒级数展开： \\[\ne^x \\approx 1 + x + \\frac{x^2}{2!} + \\frac{x^3}{3!} + \\dots + \\frac{x^k}{k!}\n\\] 对于一个有限范围和精度的输入\\(x\\)，我们总可以找到一个固定阶数\\(k\\)的多项式，使其足够精确。多项式只包含乘法和加法，根据结论1，这完全可以在\\(TC^0\\)中实现。电路的深度仅取决于这个固定阶数\\(k\\)，而与序列长度\\(n\\)无关，因此仍然是常数深度。\n模拟求和 \\(∑\\) 和归一化: \\[\n\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}}\n\\] 分母的求和\\(\\sum_{j=1}^n\\)看起来是顺序的，但强大的阈值门再次发挥作用。计算\\(n\\)个数的和可以在\\(TC^0\\)中高效完成。一个巨大的阈值门网络可以在常数深度内计算出这个和（或者其足够精确的近似值）。最终，用近似的\\(exp(z_i)\\)除以近似的\\(\\sum exp(z_j)\\)，整个Softmax操作也被一个\\(TC^0\\)电路成功模拟。\n\n\n\n2.3 整合：一个完整的\\(TC^0\\)自注意力电路\n我们将上述模块组合起来： 1. 输入层: 将浮点数输入（词嵌入）编码为固定长度的二进制串。 2. 线性变换层: 实现\\(XW_Q\\), \\(XW_K\\), \\(XW_V\\)的电路模块（基于\\(TC^0\\)乘法和加法）。 3. 注意力分数层: 实现\\(QK^T\\)和缩放的电路模块。 4. Softmax层: 实现Softmax近似的电路模块。 5. 输出层: 实现最终加权求和\\((...)V\\)的电路模块。\n由于每个模块都是\\(TC^0\\)的（常数深度，多项式大小），将它们顺序连接起来，总的电路深度仍然是常数（\\(O(1) + O(1) + ... = O(1)\\)），总大小也是多项式。\n因此，我们成功地证明了：任何一个标准自注意力层的计算过程，都可以被一个\\(TC^0\\)电路以任意给定的精度进行模拟。 (这一结论的严格数学证明可以在相关学术论文中找到，如 William Merrill et al., 2021, “A Formal Hierarchy of RNN and Transformer Architectures”）。\n\n\n\n\n第三步：深刻启示——“等价于\\(TC^0\\)”到底意味着什么？\n这个理论结论不是学术游戏，它深刻地揭示了Transformer的本质、优势和固有的“基因缺陷”。\n\n3.1 优势：大规模并行的理论基石\n\\(TC^0\\)的“常数深度”特性，完美解释了为什么Transformer能在GPU上如此成功。因为计算的逻辑深度是固定的，与序列长度\\(n\\)无关，所以无论序列多长，理论上整个注意力计算都可以在一步（或常数步）并行操作中完成。这与RNN中\\(t\\)时刻必须等待\\(t-1\\)时刻完成计算的\\(O(n)\\)深度形成了鲜明对比。GPU这种拥有数千个并行核心的硬件，正是为执行\\(TC^0\\)这类计算而生的。\n\n\n3.2 局限：无法逾越的计算天花板\n“没有免费的午餐”。获得大规模并行性的代价，就是被限制在\\(TC^0\\)的表达能力内。有很多看似简单的问题，是\\(TC^0\\)无法解决的。\n\n奇偶校验 (Parity): 判断一个二进制输入中“1”的个数是奇数还是偶数。这个问题无法由\\(TC^0\\)电路解决。这暗示Transformer在需要精确计数，尤其是对整体数量进行奇偶判断时会遇到困难。这可以推广到许多需要对输入进行精确聚合统计的任务。\n有向图可达性 (Reachability): 判断一个图中从节点A到节点B是否存在一条路径。这是一个典型的需要递归/迭代推理的问题（“我能到邻居，邻居能到它的邻居，……”），其计算深度与路径长度有关。\\(TC^0\\)的常数深度无法处理这种依赖于输入的计算深度。这暗示Transformer在处理需要多步、环环相扣的逻辑推理链（\\(A→B→C→D\\)）时存在理论困难。这就是为什么对于复杂问题，我们常常需要借助“思维链（Chain-of-Thought）”这种外部脚手架，诱导模型一步步地进行顺序推理，以弥补其并行架构在深度推理上的不足。\n置换组合 (Permutation Composition): 如Deltaformer论文中提到的追踪\\(n\\)个元素的交换。这类任务需要精确追踪每个元素的位置变化，其内在的代数结构也超越了\\(TC^0\\)的能力范围。\n\n\n\n结论\nTransformer的强大，源于它将语言建模这个看似复杂的任务，巧妙地用\\(TC^0\\)这样一种高度并行的计算模型进行了拟合。它的成功，是算法与硬件（GPU）协同进化的典范。然而，它的局限也同样深刻地烙印在其\\(TC^0\\)的基因中，使其在面对需要深度、顺序、迭代推理的任务时，显得力不从心。\n理解了这一点，就能明白为什么会有像Deltaformer（引入\\(NC¹\\)能力的矩阵求逆）这样的工作出现。它们的目标，正是在不完全牺牲并行性的前提下，挣脱\\(TC^0\\)的枷锁，向着表达能力更强的计算层级攀登，从而构建出下一代更强大的语言模型。"
  },
  {
    "objectID": "posts/一篇引起注意的HRM/index.html",
    "href": "posts/一篇引起注意的HRM/index.html",
    "title": "解构与重思：HRM论文背后的Transformer特例、动态优化与泛化边界",
    "section": "",
    "text": "近半年来，关于递归Transformer的研究层出不穷， 我之前重点关注过的有两篇。 Recurrent Depth Approach (Geiping et al., 2025) 使用KL 散度（一种衡量两次概率分布差异的指标）比较第 i 次和第 i-1 次循环得到的“下一步 token 概率分布”。如果差异已经很小（低于 5 × 10⁻⁴）则立刻停止循环，直接采样输出token；Mixture-of-Recursions (He et al., 2025) 则将不同递归深度视为专家，进行自适应的计算分配。\n近日，一篇同属于递归Transformer研究的论文(Hierarchical Reasoning Model)(HRM)论文及其在ARC-AGI基准上的卓越表现引发了AI社区的广泛关注。然而，在一系列深入的讨论和ARC官方的实证分析之后，剥离其“大脑启发”的叙事光环，更清晰地审视其技术内核。本文将基于对HRM核心机制的深度剖令，提出一个核心观点：和以上两篇论文相似，HRM在本质上可以被理解为一个标准Transformer架构的巧妙特例，其真正的创新之处不在于模拟大脑，而在于一种动态的、递归式的优化范式。\n\n第一部分：H-L层——一种时间尺度上的稀疏与密集注意力融合\n观点一: H-L层的交替运作，在本质上是一种在不同时间尺度上对信息进行处理的Transformer架构融合。其中，L-Module扮演了“密集全注意力计算”的角色，而H-Module则执行了对L-Module计算结果的“稀疏时间提取。\n\nL-Module (低层模块)的角色：密集的全注意力计算 (Dense Full Attention) 在HRM的运作中，\\(f_L\\)在每个H-cycle内会高频次地执行\\(T\\)次。每一次执行，它都对完整的输入序列状态进行一次标准的full attention计算。这可以被视为一种精细化的、局部的、密集的信息处理过程。它的目标是在一个固定的高层“规划”（由\\(f_H\\)提供）指导下，通过反复的全局信息交换，让系统状态收敛到一个局部的、细节丰富的“最优解”。\nH-Module (高层模块)的角色：稀疏的时间尺度提取 (Sparse Temporal Extraction) 与\\(f_L\\)的密集计算不同，\\(f_H\\)的激活是稀疏的。它只在\\(f_L\\)完成了一个完整的计算周期后才被调用一次。它的输入，是\\(f_L\\)经过\\(T\\)次迭代后得到的最终状态。这个行为在信息处理的意义上，可以被解读为：\\(f_H\\)不对每一个微小的中间状态进行响应，而是对一个已经“阶段性收敛”的、信息量更丰富的宏观状态进行一次提炼和抽象。因此，您所说的“对低层次注意力的稀疏提取”，可以理解为\\(f_H\\)在时间维度上对\\(f_L\\)的计算结果进行了一次稀疏采样，只取其终点，从而进行更宏观的、战略性的状态调整。\n\n这种“密集-稀疏”的交替工作模式，在架构上实现了一种计算资源的动态分配：大部分算力被用于底层的细节打磨，而少部分、但更关键的算力被用于高层的方向调整。\n观点二:HRM的递归式垂直结构，使其可以被抽象地视为一个在深度/时间维度上重复堆叠的、参数共享的Transformer Block。其每一轮递归的输出，都可以被理解为一次“隐式线性CoT”的Token吐出。因此，HRM的网络架构，本质上是一个普通双向Transformer架构的特例。\n\n垂直堆叠与参数共享: HRM的“外部循环”（Outer Loop）让整个H-L模块组被反复调用，并将上一轮的输出作为下一轮的输入。这在计算图上，等效于将一个固定的、参数共享的Transformer Block在“深度”或“时间”维度上垂直堆叠了多次。一个标准的深度Transformer是在空间上堆叠不同的Block，而HRM是在时间上重复使用同一个Block。\n隐式的线性CoT: 在LLM的CoT中，模型生成一个Token，这个Token作为下一步推理的显式“锚点”。在HRM的递归过程中，每一次循环结束后产生的整个隐藏状态张量，就可以被视为一个高维、连续的“潜空间Token”。它不像语言Token那样可读，但它在功能上扮演了同样的角色：承载上一轮推理的全部信息，并作为下一轮推理的起点。由于这个过程是确定性的、一步接一步的，所以称之为“线性CoT”是恰当的。\n作为Transformer的特例: 综合以上两点，我们可以得出结论：\n\n一个标准的、深度为\\(D\\)的双向Transformer，可以看作是\\(D\\)个不同的Block串联。\nHRM（连同其外部循环），可以看作是一个单一的、更复杂的Block（内部包含H-L交替），被参数共享地串联了\\(M\\)次（\\(M\\)为循环数）。\n从这个角度看，HRM确实是一个普通Transformer的特例——一个在递归深度上进行扩展，并采用了特定参数共享策略的Transformer。\n\n\n这也解释了ARC官方的实验结果：当替换为一个参数量和计算量（通过调整层数和循环次数）相当的普通Transformer时，性能并未出现大幅变化。因为两者在本质上都是通过增加计算深度（无论是通过堆叠不同层还是重复相同层）来提升模型容量和表达能力，只是实现路径不同。\n\n\n\n第三部分：一步梯度优化——动态掩码下的On-Policy式学习\n观点三: “一步梯度”优化，在效果上等同于一种动态掩码机制，即只对最新产生的“隐式CoT”（最近一次迭代的计算结果）进行梯度优化。这种逐轮迭代、即时更新的模式，可以被精确地类比为强化学习中的On-Policy Rollout过程。\n\n动态掩码的视角: “一步梯度”通过torch.no_grad()和.detach()，在效果上等同于在每次反向传播时，都将历史状态（过去的所有“隐式CoT”）进行“掩码”，使其不接收梯度。梯度流动的范围被严格限定在最新产生的那一个“隐式CoT”的计算过程中。随着外部循环的推进，这个“未掩码”的窗口也在动态地向前滑动。\nOn-Policy Rollout的类比:\n\n一次外部循环 (Segment) 可以被视为强化学习中的一次Rollout。模型从一个状态开始，执行一系列动作（内部的H-L计算），最终到达一个终止状态。\n计算损失并更新: 在Rollout结束后，HRM会立即计算损失（复合损失可以看作是RL中的策略梯度损失和价值损失的结合体）并更新模型参数。\nOn-Policy特性: 下一个Segment的Rollout，将从上一个Segment的最终状态开始，并且使用的是已经更新过的、最新的策略。这完全符合On-policy RL的核心定义：用于生成数据的策略和被优化的策略是同一个。\n\n\n这种“Rollout-Optimize-Continue”的循环，与标准的“采样一批-优化多次”的Off-policy RLHF算法（如PPO）形成了鲜明对比。它是一种反馈更及时、策略始终保持最新的在线学习范式。\n\n\n\n第四部分：性能归因与泛化边界\n观点四: 实证数据表明，HRM的绝大部分性能来自于在训练时见过了评估任务的“示例对”。其运作模式更接近于一种高效的“测试时训练器”或模式匹配器，而非我们所期待的、具备通用跨任务推理能力的模型。\n详细阐述与补充:\n这是ARC官方报告给出的最关键的结论，它为HRM的成功进行了“祛魅”，也为我们理解其适用范围划定了清晰的边界。\n\n测试时训练 (Test-Time Training): 报告指出，HRM的流程与Liao and Gu的“无预训练ARC-AGI”方法在哲学上是相似的。这种方法的核心是，在面对一个新任务时，利用其提供的少量示例对（demonstration pairs），通过密集的梯度下降，将解决该特定任务的“程序”或“逻辑”直接“烧录”进模型权重中。模型本身充当了一个通用的、可微分的“程序基板”。\n记忆 vs. 推理: HRM的强大之处，可能不在于它学会了通用的、可迁移的抽象推理规则，而在于它是一个极其高效的优化器/记忆器。其递归式的精炼结构，使其能够在一个很小的参数空间内，通过梯度下降快速找到能够拟合当前任务示例的权重配置。换句话说，这是一类内置了结构bias的模型，相对，其泛化能力堪忧。\n泛化能力的局限: 这一发现解释了为什么HRM在ARC-AGI-1（训练和评估任务有重叠）上表现出色，而在更考验泛化能力的ARC-AGI-2上表现不佳。它也揭示了模型的一个重大限制：其性能高度依赖于在训练时能够接触到目标任务的某种形式（例如，通过其示例对）。对于一个全新的、从未见过的任务，其表现可能会大幅下降。\n\n\n\n结论：\n它并非一个革命性的、模拟大脑的全新物种，而更像是一个在现有Transformer框架内，通过巧妙的结构特例化（时间尺度上的注意力和递归式参数共享）和新颖的优化范式（动态的、On-Policy式的迭代精炼），实现的高效“测试时优化器”。"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mythos",
    "section": "",
    "text": "解构与重思：HRM论文背后的Transformer特例、动态优化与泛化边界\n\n\n\npaper\n\n\n\n\n\n\n\n\n\nAug 20, 2025\n\n\n杨诚操\n\n\n\n\n\n\n\n\n\n\n\n\n深入剖析Deltaformer，探寻超越Transformer的计算新范式\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nAug 17, 2025\n\n\n杨诚操\n\n\n\n\n\n\n\n\n\n\n\n\nTransformer计算能力等价于\\(TC^0\\)\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nAug 17, 2025\n\n\n杨诚操\n\n\n\n\n\n\n\n\n\n\n\n\nLLM推理的“瘦身革命”：少即是多，还是新的枷索？——深入剖析GFPO算法及其数学原理\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nAug 15, 2025\n\n\n杨诚操\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nAug 15, 2025\n\n\nHarlow Malloc\n\n\n\n\n\n\n\n\n\n\n\n\n从理论断层到统一框架：LLM强化学习对齐算法深度研究（一）\n\n\n\nnews\n\ncode\n\nanalysis\n\n\n\n\n\n\n\n\n\nAug 15, 2025\n\n\n杨诚操\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/LLM推理的瘦身革命：少即是多，还是新的枷索？深入剖析GFPO算法及其数学原理/index.html",
    "href": "posts/LLM推理的瘦身革命：少即是多，还是新的枷索？深入剖析GFPO算法及其数学原理/index.html",
    "title": "LLM推理的“瘦身革命”：少即是多，还是新的枷索？——深入剖析GFPO算法及其数学原理",
    "section": "",
    "text": "在追求更强大、更聪明的语言模型（LLM）的道路上，我们常常陷入一个怪圈：模型在解决复杂推理问题时，似乎越来越“话痨”。它们生成的答案越来越长，虽然有时能换来准确率的提升，但更多时候，这些冗长的“思考过程”充满了重复、犹豫和无关紧要的填充物。这不仅拖慢了推理速度，也增加了我们对模型可控性的担忧。\n最近，一篇名为《Sample More to Think Less》的论文提出了一种名为GFPO (Group Filtered Policy Optimization) 的新方法，直指这一“长度膨胀”的痛点。它宣称能让模型在训练时“多采样”，从而在推理时“少思考”，实现了一场推理效率的“瘦身革命”。\n这听起来很美妙，但它真的如此完美吗？它是否只是用一种新的偏见替换了旧的问题？在这篇博客中，我们将以一名严谨研究者的视角，从核心原理到潜在风险，再到未来的可能性，对GFPO进行一次彻底的剖析。\n\n一、问题的核心：GRPO的“内卷”与数学瓶颈\n要理解GFPO，我们必须先了解它的前辈——GRPO (Group Relative Policy Optimization)。GRPO是当前主流的强化学习对齐算法之一，它通过让模型为每个问题生成一组（Group）答案，并基于这组答案内部的相对好坏（奖励高低）来进行学习，从而摆脱了对独立价值模型的依赖。\nGRPO的目标是最大化一个代理目标函数，其核心是优势函数\\(A_i\\)的定义： \\[\nA_i = \\frac{R(q, o_i) - \\operatorname{mean}_{j=1}^G\\{R(q, o_j)\\}}{\\operatorname{std}_{j=1}^G\\{R(q, o_j)\\}}\n\\] 其中，\\(R(q, o_i)\\)是响应\\(o_i\\)的总奖励，\\(G\\)是组内样本总数。这个公式巧妙地利用组内均值和标准差对奖励进行归一化，从而得到一个相对的“优势”分数。\n然而，这个设计有一个意想不到的副作用。在优化模型以获得更高奖励（例如，解题正确）的过程中，它往往会无意中鼓励更长的答案。因为一个详尽的、包含多种尝试的答案，更有可能“撞对”最终的正确解。这种机制导致了模型的“内卷”：为了提高正确率，不惜一切代价增加思考步骤，最终导致响应长度的失控。\n\n\n二、GFPO的巧妙一击：从“奖励塑造”到“数据筛选”\nGFPO的解决方案出人意料地简单。它在GRPO的基础上增加了一个看似微不足道却至关重要的步骤：过滤 (Filtering)。\n其工作流程如下： 1. 采样更多 (Sample More): 针对一个问题，不再只生成\\(G\\)个答案，而是生成更多，比如\\(G'\\)个（\\(G' &gt; G\\)）。 2. 设定标准 (Define Metric): 设定一个我们关心的“好”答案的标准metric(·)，这个标准可以独立于最终的任务奖励\\(R\\)。论文中主要使用了两个标准：响应长度（越短越好）和Token效率（\\(R(o)/\\text{length}(o)\\)，越高越好）。 3. 无情筛选 (Filter): 根据metric(·)，从生成的\\(G'\\)个答案中，只挑选出最符合标准的前\\(k\\)个，构成精英子集\\(S\\)。 4. 专注学习 (Focus & Learn): 完全抛弃那些不属于\\(S\\)的答案，只在被选中的这\\(k\\)个“精英”答案内部进行GRPO式的相对学习。\n这套机制的精髓在于，它将对“简洁性”的追求，从复杂的奖励函数设计中解耦出来，转化成了一个简单粗暴的“数据选择”问题。\n数学上，这是通过一个掩码 (Mask) \\(m_i\\) 实现的。GFPO修改了优势函数的计算方式： \\[\nA_{i}^{(m)} = \\frac{R(q, o_i) - \\mu_S}{\\sigma_S} \\cdot m_i\n\\] 其中： * \\(m_i = \\mathbb{I}\\{o_i \\in S\\}\\)，即如果\\(o_i\\)在精英子集\\(S\\)中，\\(m_i=1\\)，否则\\(m_i=0\\)。 * \\(\\mu_S\\)和\\(\\sigma_S\\)分别是精英子集\\(S\\)内部的奖励均值和标准差。\n对于被抛弃的样本，\\(m_i=0\\)，因此它们的优势函数\\(A_{i}^{(m)}\\)被强制设为0。在策略梯度算法中，策略更新的梯度\\(\\nabla_\\theta J\\)正比于优势函数： \\[\n\\nabla_\\theta J(\\theta) \\approx \\mathbb{E} \\left[ A^{(m)} \\cdot \\nabla_\\theta \\log \\pi_\\theta(o) \\right]\n\\] 当优势函数\\(A^{(m)}\\)为0时，该样本对梯度的贡献也精确为0——它在参数更新中被彻底“无视”了。\n\n\n三、深刻的质疑：这是优化还是“偏见注入”？\nGFPO在实验中取得了显著成功，它在大幅削减响应长度的同时，几乎没有损失准确率。但作为批判性的思考者，我们必须提出质疑：这种“成功”的代价是什么？\n我们的深度对话揭示了GFPO方法论中一个深刻的内在矛盾：它本质上是在系统性地丢弃大量负样本。\n这引发了两个核心问题： 1. 对RL逻辑的挑战：强化学习的核心是“试错”，即从成功和失败中共同学习。GFPO拒绝从它认为“不符合标准”的失败（甚至某些成功）案例中学习，这是否违背了RL的基本原则？\n\n策略空间的扭曲：GFPO的过滤行为，实际上抬高了“好”答案的基线（用\\(\\mu_S\\)代替\\(\\mu_G\\)）。一个在全局看起来还不错的答案，在“精英圈”里可能就成了差生，从而受到惩罚。这会推动模型的策略空间向一个被特定metric严重“扭曲”的区域收敛。\n\n我们的结论是：GFPO并非一个纯粹的优化算法，而是一种强大的、隐式的“偏见注入”工具。 它将我们对“简洁性”的偏好，通过数据选择的方式，强行注入到模型的学习过程中。这种“扭曲”是有意为之的，其目标就是塑造一个在特定约束下表现更优的模型。\n\n\n四、从修正到超越：GFPO范式的未来\n认识到GFPO的本质后，我们可以从两个方向探索其未来：\n1. 如何修正其“粗暴”？ GFPO的“硬截断”（\\(A^{(m)}=0\\)）丢弃了太多信息。我们可以设计更“温柔”的修正方案： * 软性惩罚: 对于被抛弃的样本\\(o_j \\notin S\\)，不将优势设为0，而是给一个固定的负值\\(A_{j}^{(m)} = -\\lambda_{rej}\\)。这样既保留了负反馈信号，又明确了对不符合metric行为的惩罚。 * 分层学习: 对“精英圈”\\(S\\)和“圈外”\\(S^c\\)的样本使用不同的学习基准和目标。例如，圈内用\\(A_{i}^{(m)} = \\frac{R(q, o_i) - \\mu_S}{\\sigma_S}\\)，圈外用\\(A_{j}^{(m)} = \\frac{R(q, o_j) - \\mu_G}{\\sigma_G} - \\delta\\)，从而充分利用所有样本信息。 * 退火机制: 在训练初期让模型自由探索（接近GRPO），后期再逐渐加强过滤偏见（接近GFPO），实现更稳定的学习。\n2. 如何利用其“偏见”？ 既然GFPO是注入偏见的利器，它的应用就可以远不止于“瘦身”。 * 多目标优化: 我们可以设计一个融合了简洁性、事实性、安全性的复杂metric函数，用GFPO这一简单框架实现复杂的多目标对齐。 * 安全对齐: 使用安全分类器作为metric，GFPO可以强力推动模型规避有害内容的生成。 * 高质量偏好学习: 与DPO等方法结合，先用GFPO筛选出高质量的候选答案，再在这些“优等生”中进行细微的偏好学习，从而训练出更具辨别力的模型。\n\n\n结论：一把双刃剑\nGFPO无疑是LLM对齐领域一项巧妙且务实的工作。它以一种全新的视角，将复杂的“奖励工程”问题转化为了更易于操作的“数据选择”问题，并取得了显著的工程效果。\n然而，我们必须清醒地认识到，GFPO是一把双刃剑。它所注入的“偏见”在当前看来利大于弊，成功为臃肿的LLM推理实现了“瘦身”。但这种对部分经验的“选择性失明”，可能会成为模型通往更高级、更鲁棒通用智能的“新枷锁”。\n理解GFPO，不仅是学习一种新算法，更是引发我们对人与机器对齐方式的深刻反思：我们是该设计一个完美的奖励函数去引导它，还是该为它设定一个严格的“朋友圈”，让它在潜移默化中学会我们所期望的样子？GFPO，为后一种可能性打开了一扇引人深思的大门。"
  },
  {
    "objectID": "posts/Transformer计算能力等价于$TC^0$/index.html",
    "href": "posts/Transformer计算能力等价于$TC^0$/index.html",
    "title": "深入剖析Deltaformer，探寻超越Transformer的计算新范式",
    "section": "",
    "text": "摘要\n介绍了一种名为Deltaformer的新型模型架构，其核心目标是结合Transformer的高度并行性与超越其\\(TC^0\\)计算表达能力的更强性能。作者团队通过融合经典的Delta Rule（一种状态更新机制）与核技巧（Kernel Trick），并设计了一种高效的块并行（Chunk-wise）算法，使得模型在理论上达到了\\(NC^1\\)复杂度类，从而能够解决Transformer难以处理的状态追踪和图连通性等问题。\n接下来，我将分步对文章中的核心概念、方法、理论和实验进行详细注解与分析。\n\n\n\n1. 动机剖析 (Motivation Analysis)\n文章的动机建立在四个环环相扣的论点上，逻辑清晰，直指当前大模型架构的核心局限。\n\n1.1 表达性与并行性的内在矛盾\n这是一个非常深刻的切入点，源于计算复杂性理论。\n\n计算复杂性类 (Complexity Classes): 为了理解这个矛盾，我们必须先厘清几个关键的复杂性类：\n\nP (Polynomial Time): 可以在确定性图灵机上通过多项式时间解决的决策问题集合。这基本涵盖了所有我们认为“可计算”的问题。循环神经网络（RNNs），如LSTM，其计算过程是顺序的，属于P类问题。\nNC (Nick’s Class): 可以在多项式数量的处理器上，在多对数（polylogarithmic, \\(\\log^k(n)\\)）时间内解决的问题集合。\\(NC\\)代表了那些可以被高效并行化的问题。\n\\(TC^0\\): NC的一个子类。它代表可以被一个常数深度、多项式大小、且包含阈值门（Threshold Gates）的电路解决的问题。标准Transformer的自注意力机制（Self-Attention）已被证明其计算能力等价于\\(TC^0\\)。阈值门可以计算输入的加权和是否超过某个阈值，这与注意力中的加权求和与Softmax操作有直接关联。\n\n核心矛盾: 作者指出，\\(TC^0\\)的能力是有限的。例如，它无法有效解决需要多步推理或状态追踪的问题。而LSTM这类顺序模型虽然表达能力更强（理论上是图灵完备的），但其计算过程是\\(t\\)依赖于\\(t-1\\)，无法大规模并行，这在GPU时代是致命的。作者认为，在几乎完全并行的\\(TC^0\\)（Transformer）和完全顺序的\\(P\\)（LSTM）之间，存在着广阔的中间地带（如\\(NC^1\\), \\(NC²\\)等），这些复杂性类既允许高度并行，又具备比\\(TC^0\\)更强的表达能力。Deltaformer的目标就是探索这片区域。\n\n\n\n1.2 Delta Rule的复苏\n\nDelta Rule: 这并非一个新概念，其思想根源可追溯至上世纪80-90年代的“快速权重编程”（Fast Weight Programming）。其核心思想是，模型的“权重”或“状态”在处理序列时是动态演化的。每一次输入不仅仅是用来计算输出，也是用来修改模型自身的状态。其一般形式可以抽象为： \\[\nS_t = f(S_{t-1}, x_t)\n\\] 其中\\(S_t\\)是\\(t\\)时刻的状态，\\(x_t\\)是\\(t\\)时刻的输入。这与Transformer的注意力机制有本质区别：Transformer在处理一个序列时，其参数\\(W_Q, W_K, W_V\\)是固定的；它通过位置编码和注意力矩阵来感知序列顺序，但没有一个随时间演化的“状态矩阵”。\n并行化挑战: 传统的Delta Rule实现是递归的，因此难以在GPU上并行。作者引用了Yang et al. (2024) 的工作，该工作成功地将一类Delta Rule（DeltaNet）并行化，使其从理论走向实践，这是Deltaformer诞生的关键催化剂。\n\n\n\n1.3 架构融合\n作者明确了Deltaformer的设计哲学：取长补短。 * Transformer: 强于长距离依赖建模和信息检索（得益于其全局的注意力感受野）。 * DeltaNet (\\(NC^1\\)模型): 强于状态追踪（State tracking），但可能受限于有限状态空间，长文记忆不如Transformer。 * Deltaformer: 旨在融合两者的优点，成为一个表现力全面超越Transformer的架构。\n\n\n\n\n2. 方法论与数学推导 (Methodology & Mathematical Derivation)\nDeltaformer的核心方法是将Delta Rule与核技巧结合，并通过一个块状算法实现高效计算。\n\n2.1 Kernelized Delta Rule\n\n核技巧 (Kernel Trick): 核技巧允许我们在一个高维甚至无穷维的特征空间中进行计算，而无需显式地定义这个空间的映射\\(\\phi(\\cdot)\\)。我们只需要定义核函数\\(K(x, y) = \\langle \\phi(x), \\phi(y) \\rangle\\)，即两个向量在高维空间中的內积。\n推导: 论文中的数学表述有些跳跃，我将尝试重构并推导一个更清晰的版本。让我们假设一个基础的Delta Rule状态更新，其状态\\(W\\)是一个矩阵： \\[\nW_t = W_{t-1} + v_t k_t^T\n\\] 在引入核技巧后，所有向量都被映射到高维空间\\(\\phi(\\cdot)\\)： \\[\n\\Phi_t = \\Phi_{t-1} + \\phi(v_t) \\phi(k_t)^T\n\\] 当需要用这个状态进行“读”操作时，我们用查询向量\\(q_t\\)去查询： \\[\ny_t = \\Phi_{t-1} \\phi(q_t) = \\left( \\sum_{i=1}^{t-1} \\phi(v_i) \\phi(k_i)^T \\right) \\phi(q_t)\n\\] 根据线性代数，上式可以变为： \\[\ny_t = \\sum_{i=1}^{t-1} \\phi(v_i) \\left( \\phi(k_i)^T \\phi(q_t) \\right)\n\\] 此时，我们可以应用核函数\\(K(k_i, q_t) = \\phi(k_i)^T \\phi(q_t)\\)来替换无穷维的內积： \\[\ny_t = \\sum_{i=1}^{t-1} K(k_i, q_t) \\phi(v_i)\n\\] 这个结果\\(y_t\\)仍然是无穷维的。论文在这里的描述（“将\\(\\phi\\)和\\(\\Psi\\)都给消去了”）可能存在一定的模糊性。一个可能的解释是，最终的输出\\(o_t\\)是通过另一次核化的方式计算的，例如\\(o_t = \\sum_j K(y_t, z_j) \\alpha_j\\)，从而避免了无穷维向量的直接操作。\n论文中的公式: 论文中呈现的读写公式更为复杂，暗示了一种更高级的更新规则，但其核心思想依然是利用核函数替换內积。作者选择\\(softmax\\)作为核函数，即\\(K(q, k) = \\text{softmax}(q^T k)\\)，这巧妙地将该机制与Transformer的注意力形式统一起来。\n\n\n\n2.2 Chunk-wise Algorithm for GPU Implementation\n这是Deltaformer能够高效训练的关键。其核心是计算一个随时间演化的矩阵的逆，即\\(U_t = (I - \\sum_{i=1}^t v_i k_i^T)^{-1}\\)。\n\n问题: 直接计算\\(U_t\\)需要\\(O(t d^2)\\)的计算量和\\(O(d^2)\\)的内存，并且是递归的，难以并行。\n块状计算思想: 将长度为\\(L\\)的序列划分为大小为\\(B\\)的块（chunk）。我们希望能够基于前一个块的最终状态\\(U_p\\)，高效地并行计算当前块内的状态\\(U_c\\)。\n数学推导: 让我们定义 \\(A_t = \\sum_{i=1}^t v_i k_i^T\\)。\n\n\\(A_t = A_p + A_c\\)，其中\\(A_p\\)是之前所有块的\\(vk^T\\)之和，\\(A_c\\)是当前块的\\(vk^T\\)之和。\n目标是计算 \\(U_t = (I - A_p - A_c)^{-1}\\)。\n这里我们可以使用Woodbury矩阵恒等式或其特例Sherman-Morrison公式。该恒等式描述了矩阵的逆在受到一个低秩更新时的变化： \\[\n(A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}\n\\]\n我们将 \\(I - A_p - A_c\\) 变形为\\((I - A_p) - A_c\\)。令\\(B = I - A_p\\)，则\\(B^{-1} = U_p\\)。我们要求\\((B - A_c)^{-1}\\)。\n\\(A_c\\)可以表示为\\(V_c K_c^T\\)，其中\\(V_c\\)和\\(K_c\\)分别是当前块的\\(v\\)和\\(k\\)向量堆叠成的矩阵。\n应用Woodbury恒等式： \\[\nU_t = (B - V_c K_c^T)^{-1} = B^{-1} + B^{-1}V_c(I - K_c^T B^{-1} V_c)^{-1} K_c^T B^{-1}\n\\] 代入\\(B^{-1} = U_p\\)： \\[\nU_t = U_p + U_p V_c (I - K_c^T U_p V_c)^{-1} K_c^T U_p\n\\] 这个公式允许我们从前一个块的状态\\(U_p\\)和当前块的数据\\((V_c, K_c)\\)来计算当前块的最终状态\\(U_t\\)。其中，\\(I - K_c^T U_p V_c\\)是一个\\(B \\times B\\)的小矩阵，其求逆成本较低。当前块内部的计算可以高度并行化。\n关于论文中公式的评论: 论文中给出的更新公式 \\(U_c = (I - A_c)^{-1} (I + A_c U_p)\\) 似乎与标准的Woodbury恒等式推导出的形式不同。这可能是一个简化或特定条件下的表述，也可能是原文笔误。标准的推导如上所示，它清晰地展示了如何从\\(U_p\\)更新到\\(U_t\\)。然而，无论具体公式形式如何，其核心思想是将对\\(t \\times t\\)大矩阵的依赖，转化为对\\(d \\times d\\)状态矩阵和\\(B \\times B\\)块内交互矩阵的操作。\n\n复杂度分析: 作者给出的\\(O(L/B \\cdot (B^2 d + B d^2 + d^3))\\)是合理的。\n\n\\(B^2 d\\): 计算块内的\\(K^T U_p V\\)等矩阵乘法。\n\\(B d^2\\): 块内其他矩阵乘法。\n\\(d^3\\): 每次块更新结束时，可能需要一个\\(d \\times d\\)的矩阵求逆（或类似操作）来稳定或更新状态。 这个复杂度允许模型以块为单位进行并行，实现了在GPU上的高效训练。\n\n\n\n\n\n\n3. 理论分析 (Theoretical Analysis)\nDeltaformer被证明能够达到\\(NC^1\\)的表达能力。\n\n\\(NC^1\\)的意义: \\(NC^1\\)比\\(TC^0\\)更强大。\\(NC^1\\)能够解决而\\(TC^0\\)无法解决的典型问题包括：\n\n奇偶校验 (Parity): 判断输入二进制位中1的个数是奇数还是偶数。\n有向图可达性 (Graph Reachability): 判断图中两个节点之间是否存在路径。\n置换组合 (Permutation Composition): 如论文中提到的追踪n个元素的交换。\n\n证明思路 (构造性证明):\n\n选择一个\\(NC^1\\)完备问题: 作者选择了“追踪n个元素交换”这一任务。如果能证明Deltaformer可以解决这个问题，就证明了它的能力至少达到了\\(NC^1\\)。\n矩阵求逆与图可达性: Deltaformer的核心操作是\\(U = (I - A)^{-1}\\)。这个操作与图论紧密相关。如果我们考虑\\(A\\)为一个图的邻接矩阵，那么\\((I-A)^{-1}\\)可以通过其诺依曼级数展开： \\[\n(I - A)^{-1} = I + A + A^2 + A^3 + \\dots\n\\] \\(A^k\\)的第\\((i, j)\\)个元素表示从节点\\(i\\)到节点\\(j\\)长度为\\(k\\)的路径数量。因此，\\(\\sum_{k=0}^{\\infty} A^k\\)的第\\((i, j)\\)个元素非零，当且仅当节点\\(i\\)和\\(j\\)之间存在路径。这直接对应了图的可达性问题。\n结论: 由于矩阵求逆（在特定代数结构下）是\\(NC\\)中的一个基本操作，且与图可达性等价，而图可达性是\\(NC^1\\)的经典问题，因此包含该操作的Deltaformer天然具备了超越\\(TC^0\\)的潜力。作者在论文中通过具体构造证明了这一点。\n\n\n\n\n\n4. 实验验证与批判性评估\n\n4.1 Toy Task的意义\n作者选择在Toy Task上进行实验，这在模型理论研究中是非常严谨和必要的。这些任务（如元素交换、图连通性）被专门设计用来探测模型的特定计算能力。如果一个模型声称具备\\(NC^1\\)的能力，它就必须能够解决这些任务。实验结果显示Deltaformer可以解决而Transformer不能，这为模型的理论主张提供了强有力的实证支持。\n\n\n4.2 批判性评估与潜在问题\n作为一个研究者，我们需要审视该工作可能存在的局限和挑战：\n\n大规模语言任务的扩展性 (Scalability to large-scale tasks): 这是最大的问题。在Toy Task上的成功并不能直接保证在真实、复杂的语言建模任务上也能取得优势。\\(NC^1\\)能力（如计算图连通性）是否是提升语言理解和生成能力的关键？这仍然是一个开放性问题。模型的归纳偏置（inductive bias）是否与自然语言的内在结构完全匹配，有待大规模实验验证。\n计算成本与效率: 尽管块状算法实现了并行，但\\(d^3\\)的求逆操作成本很高。在标准Transformer中，\\(d\\)（head dimension）通常是64或128。\\(128^3\\)约等于200万，这是一个不小的计算开销，可能会限制模型的隐藏层维度或总参数量，从而影响其容量。与FlashAttention等高度优化的注意力实现相比，Deltaformer的实际训练速度和效率需要进行详细的基准测试。\n数值稳定性 (Numerical Stability): 矩阵求逆是一个对数值敏感的操作。如果矩阵\\(I-A\\)是病态的（ill-conditioned）或接近奇异，求逆过程可能会导致巨大的数值误差，从而使训练崩溃。模型是否需要特殊的正则化手段、激活函数或者参数初始化策略来保证\\(I-A\\)的良好性质？论文中没有提及这一点，但在实际应用中至关重要。\n核函数的选择: 作者提到核函数的选择很重要。使用\\(softmax\\)核是一个巧妙的设计，因为它与现有Transformer组件兼容。但这是否是最优选择？其他核函数（如线性核、多项式核、高斯核）是否会带来不同的性能和计算特性？这方面需要更多的消融实验。\n\n\n\n\n5. 结论\n该论文清晰地阐述了Deltaformer的核心思想、理论基础和实现路径。它精准地抓住了当前主流架构Transformer的理论局限（\\(TC^0\\)），并从计算复杂性理论中寻找突破口，提出了一种更有潜力的\\(NC^1\\)模型。\n总结来说，这项工作是富有洞察力的，它将经典的计算理论与现代的深度学习架构设计相结合，逻辑严密，理论扎实。然而，其实用价值最终取决于其在大规模、真实世界任务上的表现，以及能否在计算成本和数值稳定性等工程挑战上找到令人满意的解决方案。 这确实是一块值得后来者继续挖掘的“美玉”。"
  },
  {
    "objectID": "posts/从理论断层到统一框架：LLM强化学习对齐算法深度研究（一）/index.html",
    "href": "posts/从理论断层到统一框架：LLM强化学习对齐算法深度研究（一）/index.html",
    "title": "从理论断层到统一框架：LLM强化学习对齐算法深度研究（一）",
    "section": "",
    "text": "记录了一次关于大型语言模型（LLM）强化学习（RL）对齐算法的深度研讨。我们从一个基础性的矛盾出发——即在LLM这种非遍历性序列生成任务中，Token级重要性采样的理论不自洽性。以此为起点，我们系统性地重演了从策略梯度第一性原理到现代实用算法（如PPO, GRPO）的推导路径，并揭示了其中为了实践可行性而做出的关键妥协，如序列级重要性采样（IS）的引入及其内在的方差-偏差困境。我们深入剖析了GRPO等算法存在的长度与难度偏差，并审视了DAPO和CISPO等前沿工作如何从工程和理论层面修复这些缺陷。最后，基于本次研讨的洞察，我们提出了一个旨在统一现有算法优点、并解决其核心痛点的综合性框架（SGS-CISPO），并对其数学完备性和实践潜力进行了严谨评估。\n\n第一幕： foundational Conflict - 理论的优雅与现实的诅咒\n一切LLM-RLHF的起点，都是最大化期望奖励这一简单而优美的目标： \\[\nJ(\\theta) = \\mathbb{E}_{o \\sim P_\\theta(o)}[R(o)]\n\\] 其中，\\(P_\\theta(o)\\)是策略模型\\(\\pi_\\theta\\)生成完整序列\\(o\\)的概率。通过应用Log-Derivative Trick，我们得到了著名的REINFORCE策略梯度： \\[\n\\nabla_\\theta J(\\theta) = \\mathbb{E}_{o \\sim P_\\theta(o)}[R(o) \\nabla_\\theta \\log P_\\theta(o)]\n\\] 然而，这个理论上完美的梯度在实践中存在两大缺陷：高方差和On-Policy数据低效。为了解决这两个问题，我们引入了两个标准工具：\n\n基线（Baseline）: 引入优势函数 \\(\\hat{A}(o) = R(o) - b\\) 来降低方差。\n重要性采样（Importance Sampling, IS）: 允许使用旧策略\\(\\pi_{\\text{old}}\\)采样的数据，以提高效率。\n\n将两者结合，我们得到了理论上最完备的Off-Policy策略梯度： \\[\n\\nabla_\\theta J(\\theta) = \\mathbb{E}_{o \\sim P_{\\text{old}}(o)}\\left[ \\frac{P_\\theta(o)}{P_{\\text{old}}(o)} \\cdot \\hat{A}(o) \\cdot \\nabla_\\theta \\log P_\\theta(o) \\right]\n\\] 这里的核心是序列级重要性采样比例： \\[\nR_{\\text{sequence}}(o) = \\frac{P_\\theta(o)}{P_{\\text{old}}(o)} = \\prod_{t=1}^{T} \\frac{\\pi_\\theta(o_t|o_{&lt;t})}{\\pi_{\\text{old}}(o_t|o_{&lt;t})} = \\prod_{t=1}^{T} \\rho_t\n\\] 至此，我们遭遇了第一个，也是最根本的理论-实践断裂点。\n这个连乘积\\(R_{\\text{sequence}}(o)\\)，虽然在数学上是唯一正确的权重，但在实践中是彻头彻尾的灾难。它的方差会随着序列长度\\(T\\)指数级爆炸，并导致毁灭性的数值不稳定性（梯度消失或爆炸）。理论的圣杯，在现实中却“有毒”。\n\n\n第二幕：The Age of Compromise - PPO/GRPO/GSPO的问题\n为了让训练能够进行，现代算法必须对\\(R_{\\text{sequence}}(o)\\)这个“有毒的圣杯”进行“解毒”。这催生了两种主流的妥协方案：\n方案A：PPO/GRPO的“理论混搭”\nPPO及其变体GRPO，采取了一种极其务实但理论上不纯粹的方案。它们放弃了序列级的\\(R_{\\text{sequence}}\\)，转而直接在Token级别上使用IS： \\[\nL_{\\text{PPO/GRPO}} \\propto - \\sum_{i,t} \\min\\left( \\rho_{i,t} \\cdot \\hat{A}_i, \\text{clip}(\\rho_{i,t}) \\hat{A}_i \\right)\n\\] 这里的核心问题是，它将一个序列级的优势函数\\(\\hat{A}_i\\)与一个Token级的重要性采样比例\\(\\rho_{i,t}\\)直接相乘。这在“序列即动作”的理论框架下是不自洽的，它造成了理论上的断层，但其简单的形式和鲁棒的稳定性使其成为业界标准。\n方案B：GSPO的“有偏妥协”\nGSPO试图在理论上做得更自洽。它坚守“序列即动作”的原则，但为了解决\\(R_{\\text{sequence}}\\)的方差问题，它采用了几何平均来替代： \\[\nG(o) = (R_{\\text{sequence}}(o))^{1/T} = \\left(\\prod_{t=1}^{T} \\rho_t\\right)^{1/T}\n\\] 这是第二个关键的妥协点。几何平均通过在对数空间取算术平均（\\(\\log G(o) = \\frac{1}{T}\\sum \\log \\rho_t\\)），成功地将指数增长的方差驯服为以\\(1/T\\)速率下降的方差。\n\n合理性: 保留了连乘的本质结构（最弱一环效应），且极大地稳定了训练。\n不合理性: 引入了系统性的、向下的偏差。根据AM-GM不等式，几何平均总是小于等于算术平均，这意味着它会系统性地低估真实的IS权重，尤其是在新旧策略差异较大时。\n\n此外，早期的GRPO还存在两个严重的工程缺陷，后被DAPO等工作修正： 1. 长度偏差: 由于对每个序列的损失进行平均（\\(1/|o_i|\\)），导致长序列中关键token的梯度信号被稀释。解决方案是采用Token级归一化（分母为总token数）。 2. 难度偏差: 由于对优势函数按组进行标准化（\\(/std()\\)），导致全对/全错（低方差）的组被赋予过高权重，且在std=0时梯度爆炸。解决方案是动态采样，过滤掉这些组。\n\n\n第三幕：The Reformation - CISPO的“手术刀式”修复\n在PPO/GRPO的框架下，clip机制本身也存在一个长期被忽视的缺陷。当一个关键token的IS比例\\(\\rho_t\\)因其重要性而远超\\(1+\\epsilon\\)时，PPO的损失项变为一个真正常量，其梯度瞬间归零。这扼杀了模型学习突破性创新的能力。\nCISPO通过一个极其精妙的stop_gradient操作，完美地修复了这个问题。其目标函数形如： \\[\nJ_{\\text{CISPO}}(\\theta) \\propto \\mathbb{E} \\left[ \\text{sg}(\\text{clip}(\\rho_{i,t})) \\cdot \\hat{A}_{i,t} \\cdot \\log \\pi_\\theta(o_{i,t}|...) \\right]\n\\] 其梯度为： \\[\n\\nabla_\\theta J_{\\text{CISPO}} \\propto \\text{clip}(\\rho_{i,t}) \\cdot \\hat{A}_{i,t} \\cdot \\nabla_\\theta \\log \\pi_\\theta(o_{i,t}|...)\n\\] 在这里，clip(rho)不再是梯度的“开关”，而是一个无梯度的、有界的“缩放因子”。梯度永远不会被杀死，只是其大小被限制了。CISPO不是PPO的变体，而是一个方差有界的、稳定的REINFORCE算法，它同时解决了REINFORCE的梯度爆炸和PPO的梯度消失问题。\n\n\n第四幕：The Synthesis - 一个统一、更优的框架构想\n基于以上所有讨论，我们能否设计一个集大成、同时解决各自痛点的统一框架？我们的研讨提出了这样一个构想，它建立在几个核心原则之上：\n原则一：地基——采纳DAPO的最佳实践。 * 默认使用Token级归一化消除长度偏差。 * 默认使用动态采样消除难度偏差。\n原则二：引擎——采用CISPO的稳定梯度机制。 * 抛弃PPO的clip，使用CISPO的stop_gradient机制，确保梯度信号的完整性和稳定性。\n原则三：哲学——引入平滑梯度缩放（SGS），实现智能的正负优化。 我们认识到，单纯的负向优化（如LoNSPo）存在学习效率低下和能力上限受限的问题。而单纯的正向优化又面临“过度模仿”和“熵坍塌”的风险。为此，我们设计了一个平滑梯度缩放（Smooth Gradient Scaling, SGS）机制。\n其核心是设计一个依赖于模型置信度的梯度缩放因子 \\(S(P_\\theta, \\hat{A})\\): \\[\nS(P_\\theta, \\hat{A}) =\n\\begin{cases}\n1 & \\text{if } \\hat{A} \\le 0 \\\\\n1 - \\sigma(k(P_\\theta(o) - \\tau)) & \\text{if } \\hat{A} &gt; 0\n\\end{cases}\n\\] 这个机制在数学上可以证明： 1. 对于负样本 (\\(\\hat{A} \\le 0\\)): 它不施加任何抑制，执行完全的“进化选择”，有效淘汰错误。 2. 对于正样本 (\\(\\hat{A} &gt; 0\\)): 它允许对低置信度（新探索）的样本进行充分学习，但随着模型置信度\\(P_\\theta\\)的提升，会平滑地抑制梯度，从而在利用正样本的同时，从机制上防止了熵坍塌，避免了对已有知识的“过度学习”。\n最终的统一损失函数框架 (SGS-CISPO): \\[\nL_{\\text{SGS-CISPO}}(\\theta) = - \\frac{1}{\\sum|o_i|} \\sum_{i,t} \\left[ S_{i,t} \\cdot \\text{sg}(\\text{clip}(\\rho_{i,t})) \\cdot \\hat{A}_{i,t} \\cdot \\log \\pi_\\theta(o_{i,t}|...) \\right]\n\\] 其中，\\(S_{i,t}\\)是应用于每个token的SGS缩放因子，\\(\\hat{A}_{i,t}\\)可采用LoNSPo的留一法基线以获得更精确的信号。\n\n\n结论：迈向更完备的对齐理论\n我们的研讨从一个根本性的理论矛盾出发，层层剖析了现代LLM-RLHF算法在实践中做出的种种妥协及其代价。从PPO/GRPO的理论断层，到GSPO的有偏近似，再到DAPO和CISPO的精细修复，我们看到了一条清晰的演进路径：算法正朝着更稳定、更高效、理论更完备的方向发展。\n我们最终提出的SGS-CISPO框架，正是这一演进方向的逻辑延伸。它试图在一个统一的数学形式下，同时解决梯度稳定性（CISPO）、偏差（DAPO）和探索-利用的深层矛盾（SGS）。这是否是最终的答案尚不可知，但它为未来的研究指明了一条充满希望的、值得探索的道路。"
  }
]