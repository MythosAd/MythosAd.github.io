---
title: "深入剖析Deltaformer，探寻超越Transformer的计算新范式"
author: "杨诚操"
date: "2025-08-17"
categories: [news]
---

### **摘要**

介绍了一种名为Deltaformer的新型模型架构，其核心目标是结合Transformer的高度并行性与超越其$TC^0$计算表达能力的更强性能。作者团队通过融合经典的Delta Rule（一种状态更新机制）与核技巧（Kernel Trick），并设计了一种高效的块并行（Chunk-wise）算法，使得模型在理论上达到了$NC^1$复杂度类，从而能够解决Transformer难以处理的状态追踪和图连通性等问题。

接下来，我将分步对文章中的核心概念、方法、理论和实验进行详细注解与分析。

---

### **1. 动机剖析 (Motivation Analysis)**

文章的动机建立在四个环环相扣的论点上，逻辑清晰，直指当前大模型架构的核心局限。

#### **1.1 表达性与并行性的内在矛盾**

这是一个非常深刻的切入点，源于计算复杂性理论。

*   **计算复杂性类 (Complexity Classes)**: 为了理解这个矛盾，我们必须先厘清几个关键的复杂性类：
    *   **P (Polynomial Time)**: 可以在确定性图灵机上通过多项式时间解决的决策问题集合。这基本涵盖了所有我们认为“可计算”的问题。循环神经网络（RNNs），如LSTM，其计算过程是顺序的，属于P类问题。
    *   **NC (Nick's Class)**: 可以在多项式数量的处理器上，在多对数（polylogarithmic, $\log^k(n)$）时间内解决的问题集合。$NC$代表了那些可以被高效**并行化**的问题。
    *   **$TC^0$**: NC的一个子类。它代表可以被一个**常数深度**、**多项式大小**、且包含**阈值门（Threshold Gates）**的电路解决的问题。标准Transformer的自注意力机制（Self-Attention）已被证明其计算能力等价于$TC^0$。阈值门可以计算输入的加权和是否超过某个阈值，这与注意力中的加权求和与Softmax操作有直接关联。

*   **核心矛盾**: 作者指出，$TC^0$的能力是有限的。例如，它无法有效解决需要多步推理或状态追踪的问题。而LSTM这类顺序模型虽然表达能力更强（理论上是图灵完备的），但其计算过程是$t$依赖于$t-1$，无法大规模并行，这在GPU时代是致命的。作者认为，在几乎完全并行的$TC^0$（Transformer）和完全顺序的$P$（LSTM）之间，存在着广阔的中间地带（如$NC^1$, $NC²$等），这些复杂性类既允许高度并行，又具备比$TC^0$更强的表达能力。Deltaformer的目标就是探索这片区域。

#### **1.2 Delta Rule的复苏**

*   **Delta Rule**: 这并非一个新概念，其思想根源可追溯至上世纪80-90年代的“快速权重编程”（Fast Weight Programming）。其核心思想是，模型的“权重”或“状态”在处理序列时是动态演化的。每一次输入不仅仅是用来计算输出，也是用来**修改**模型自身的状态。其一般形式可以抽象为：
    $$
    S_t = f(S_{t-1}, x_t)
    $$
    其中$S_t$是$t$时刻的状态，$x_t$是$t$时刻的输入。这与Transformer的注意力机制有本质区别：Transformer在处理一个序列时，其参数$W_Q, W_K, W_V$是固定的；它通过位置编码和注意力矩阵来感知序列顺序，但没有一个随时间演化的“状态矩阵”。

*   **并行化挑战**: 传统的Delta Rule实现是递归的，因此难以在GPU上并行。作者引用了Yang et al. (2024) 的工作，该工作成功地将一类Delta Rule（DeltaNet）并行化，使其从理论走向实践，这是Deltaformer诞生的关键催化剂。

#### **1.3 架构融合**

作者明确了Deltaformer的设计哲学：**取长补短**。
*   **Transformer**: 强于长距离依赖建模和信息检索（得益于其全局的注意力感受野）。
*   **DeltaNet ($NC^1$模型)**: 强于状态追踪（State tracking），但可能受限于有限状态空间，长文记忆不如Transformer。
*   **Deltaformer**: 旨在融合两者的优点，成为一个表现力全面超越Transformer的架构。

---

### **2. 方法论与数学推导 (Methodology & Mathematical Derivation)**

Deltaformer的核心方法是将Delta Rule与核技巧结合，并通过一个块状算法实现高效计算。

#### **2.1 Kernelized Delta Rule**

*   **核技巧 (Kernel Trick)**: 核技巧允许我们在一个高维甚至无穷维的特征空间中进行计算，而无需显式地定义这个空间的映射$\phi(\cdot)$。我们只需要定义核函数$K(x, y) = \langle \phi(x), \phi(y) \rangle$，即两个向量在高维空间中的內积。

*   **推导**: 论文中的数学表述有些跳跃，我将尝试重构并推导一个更清晰的版本。让我们假设一个基础的Delta Rule状态更新，其状态$W$是一个矩阵：
    $$
    W_t = W_{t-1} + v_t k_t^T
    $$
    在引入核技巧后，所有向量都被映射到高维空间$\phi(\cdot)$：
    $$
    \Phi_t = \Phi_{t-1} + \phi(v_t) \phi(k_t)^T
    $$
    当需要用这个状态进行“读”操作时，我们用查询向量$q_t$去查询：
    $$
    y_t = \Phi_{t-1} \phi(q_t) = \left( \sum_{i=1}^{t-1} \phi(v_i) \phi(k_i)^T \right) \phi(q_t)
    $$
    根据线性代数，上式可以变为：
    $$
    y_t = \sum_{i=1}^{t-1} \phi(v_i) \left( \phi(k_i)^T \phi(q_t) \right)
    $$
    此时，我们可以应用核函数$K(k_i, q_t) = \phi(k_i)^T \phi(q_t)$来替换无穷维的內积：
    $$
    y_t = \sum_{i=1}^{t-1} K(k_i, q_t) \phi(v_i)
    $$
    这个结果$y_t$仍然是无穷维的。论文在这里的描述（“将$\phi$和$\Psi$都给消去了”）可能存在一定的模糊性。一个可能的解释是，最终的输出$o_t$是通过另一次核化的方式计算的，例如$o_t = \sum_j K(y_t, z_j) \alpha_j$，从而避免了无穷维向量的直接操作。

*   **论文中的公式**: 论文中呈现的读写公式更为复杂，暗示了一种更高级的更新规则，但其核心思想依然是利用核函数替换內积。作者选择$softmax$作为核函数，即$K(q, k) = \text{softmax}(q^T k)$，这巧妙地将该机制与Transformer的注意力形式统一起来。

#### **2.2 Chunk-wise Algorithm for GPU Implementation**

这是Deltaformer能够高效训练的关键。其核心是计算一个随时间演化的矩阵的逆，即$U_t = (I - \sum_{i=1}^t v_i k_i^T)^{-1}$。

*   **问题**: 直接计算$U_t$需要$O(t d^2)$的计算量和$O(d^2)$的内存，并且是递归的，难以并行。
*   **块状计算思想**: 将长度为$L$的序列划分为大小为$B$的块（chunk）。我们希望能够基于前一个块的最终状态$U_p$，高效地并行计算当前块内的状态$U_c$。
*   **数学推导**: 让我们定义 $A_t = \sum_{i=1}^t v_i k_i^T$。
    *   $A_t = A_p + A_c$，其中$A_p$是之前所有块的$vk^T$之和，$A_c$是当前块的$vk^T$之和。
    *   目标是计算 $U_t = (I - A_p - A_c)^{-1}$。
    *   这里我们可以使用**Woodbury矩阵恒等式**或其特例**Sherman-Morrison公式**。该恒等式描述了矩阵的逆在受到一个低秩更新时的变化：
        $$
        (A + UCV)^{-1} = A^{-1} - A^{-1}U(C^{-1} + VA^{-1}U)^{-1}VA^{-1}
        $$
    *   我们将 $I - A_p - A_c$ 变形为$(I - A_p) - A_c$。令$B = I - A_p$，则$B^{-1} = U_p$。我们要求$(B - A_c)^{-1}$。
    *   $A_c$可以表示为$V_c K_c^T$，其中$V_c$和$K_c$分别是当前块的$v$和$k$向量堆叠成的矩阵。
    *   应用Woodbury恒等式：
        $$
        U_t = (B - V_c K_c^T)^{-1} = B^{-1} + B^{-1}V_c(I - K_c^T B^{-1} V_c)^{-1} K_c^T B^{-1}
        $$
        代入$B^{-1} = U_p$：
        $$
        U_t = U_p + U_p V_c (I - K_c^T U_p V_c)^{-1} K_c^T U_p
        $$
    这个公式允许我们从前一个块的状态$U_p$和当前块的数据$(V_c, K_c)$来计算当前块的最终状态$U_t$。其中，$I - K_c^T U_p V_c$是一个$B \times B$的小矩阵，其求逆成本较低。当前块内部的计算可以高度并行化。

    *   **关于论文中公式的评论**: 论文中给出的更新公式 $U_c = (I - A_c)^{-1} (I + A_c U_p)$ 似乎与标准的Woodbury恒等式推导出的形式不同。这可能是一个简化或特定条件下的表述，也可能是原文笔误。标准的推导如上所示，它清晰地展示了如何从$U_p$更新到$U_t$。然而，无论具体公式形式如何，其核心思想是**将对$t \times t$大矩阵的依赖，转化为对$d \times d$状态矩阵和$B \times B$块内交互矩阵的操作**。

*   **复杂度分析**: 作者给出的$O(L/B \cdot (B^2 d + B d^2 + d^3))$是合理的。
    *   $B^2 d$: 计算块内的$K^T U_p V$等矩阵乘法。
    *   $B d^2$: 块内其他矩阵乘法。
    *   $d^3$: 每次块更新结束时，可能需要一个$d \times d$的矩阵求逆（或类似操作）来稳定或更新状态。
    这个复杂度允许模型以块为单位进行并行，实现了在GPU上的高效训练。

---

### **3. 理论分析 (Theoretical Analysis)**

Deltaformer被证明能够达到$NC^1$的表达能力。

*   **$NC^1$的意义**: $NC^1$比$TC^0$更强大。$NC^1$能够解决而$TC^0$无法解决的典型问题包括：
    *   **奇偶校验 (Parity)**: 判断输入二进制位中1的个数是奇数还是偶数。
    *   **有向图可达性 (Graph Reachability)**: 判断图中两个节点之间是否存在路径。
    *   **置换组合 (Permutation Composition)**: 如论文中提到的追踪n个元素的交换。

*   **证明思路 (构造性证明)**:
    1.  **选择一个$NC^1$完备问题**: 作者选择了“追踪n个元素交换”这一任务。如果能证明Deltaformer可以解决这个问题，就证明了它的能力至少达到了$NC^1$。
    2.  **矩阵求逆与图可达性**: Deltaformer的核心操作是$U = (I - A)^{-1}$。这个操作与图论紧密相关。如果我们考虑$A$为一个图的邻接矩阵，那么$(I-A)^{-1}$可以通过其诺依曼级数展开：
        $$
        (I - A)^{-1} = I + A + A^2 + A^3 + \dots
        $$
        $A^k$的第$(i, j)$个元素表示从节点$i$到节点$j$长度为$k$的路径数量。因此，$\sum_{k=0}^{\infty} A^k$的第$(i, j)$个元素非零，当且仅当节点$i$和$j$之间存在路径。这直接对应了**图的可达性问题**。
    3.  **结论**: 由于矩阵求逆（在特定代数结构下）是$NC$中的一个基本操作，且与图可达性等价，而图可达性是$NC^1$的经典问题，因此包含该操作的Deltaformer天然具备了超越$TC^0$的潜力。作者在论文中通过具体构造证明了这一点。

---

### **4. 实验验证与批判性评估**

#### **4.1 Toy Task的意义**

作者选择在Toy Task上进行实验，这在模型理论研究中是非常严谨和必要的。这些任务（如元素交换、图连通性）被专门设计用来探测模型的特定计算能力。如果一个模型声称具备$NC^1$的能力，它就**必须**能够解决这些任务。实验结果显示Deltaformer可以解决而Transformer不能，这为模型的理论主张提供了强有力的实证支持。

#### **4.2 批判性评估与潜在问题**

作为一个研究者，我们需要审视该工作可能存在的局限和挑战：

1.  **大规模语言任务的扩展性 (Scalability to large-scale tasks)**: 这是最大的问题。在Toy Task上的成功并不能直接保证在真实、复杂的语言建模任务上也能取得优势。$NC^1$能力（如计算图连通性）是否是提升语言理解和生成能力的关键？这仍然是一个开放性问题。模型的归纳偏置（inductive bias）是否与自然语言的内在结构完全匹配，有待大规模实验验证。
2.  **计算成本与效率**: 尽管块状算法实现了并行，但$d^3$的求逆操作成本很高。在标准Transformer中，$d$（head dimension）通常是64或128。$128^3$约等于200万，这是一个不小的计算开销，可能会限制模型的隐藏层维度或总参数量，从而影响其容量。与FlashAttention等高度优化的注意力实现相比，Deltaformer的实际训练速度和效率需要进行详细的基准测试。
3.  **数值稳定性 (Numerical Stability)**: 矩阵求逆是一个对数值敏感的操作。如果矩阵$I-A$是病态的（ill-conditioned）或接近奇异，求逆过程可能会导致巨大的数值误差，从而使训练崩溃。模型是否需要特殊的正则化手段、激活函数或者参数初始化策略来保证$I-A$的良好性质？论文中没有提及这一点，但在实际应用中至关重要。
4.  **核函数的选择**: 作者提到核函数的选择很重要。使用$softmax$核是一个巧妙的设计，因为它与现有Transformer组件兼容。但这是否是最优选择？其他核函数（如线性核、多项式核、高斯核）是否会带来不同的性能和计算特性？这方面需要更多的消融实验。

### **5. 结论**

该论文清晰地阐述了Deltaformer的核心思想、理论基础和实现路径。它精准地抓住了当前主流架构Transformer的理论局限（$TC^0$），并从计算复杂性理论中寻找突破口，提出了一种更有潜力的$NC^1$模型。

**总结来说，这项工作是富有洞察力的，它将经典的计算理论与现代的深度学习架构设计相结合，逻辑严密，理论扎实。然而，其实用价值最终取决于其在大规模、真实世界任务上的表现，以及能否在计算成本和数值稳定性等工程挑战上找到令人满意的解决方案。** 这确实是一块值得后来者继续挖掘的“美玉”。